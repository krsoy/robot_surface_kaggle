{"cells":[{"metadata":{},"cell_type":"markdown","source":"**This solution was born from those elite scientists's works.**\nPlease accept my deepest aplologies if I missed someone.\n[Markus F](https://www.kaggle.com/friedchips), \n[Thomas Rohwer](https://www.kaggle.com/trohwer64), \n[Nanashi](https://www.kaggle.com/jesucristo) \n[The Missing Link](https://www.kaggle.com/friedchips/the-missing-link)\n[\"The Orientation Sensor\" or \"Science vs. Alchemy\" discussion](https://www.kaggle.com/c/career-con-2019/discussion/87239#latest-512162)\n[Smart Robots. Complete Notebook](https://www.kaggle.com/jesucristo/1-smart-robots-complete-notebook-0-73).\n\n\nI'm a true beginner in machine learning, and I'm a self-learner.\nI want to learn, I need to learn, and I really want a job, this competition is 'fit' for me.\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nfrom time import time\nfrom sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom matplotlib import rcParams\n# %matplotlib inline\nle = preprocessing.LabelEncoder()\nfrom numba import jit\nimport itertools\nfrom seaborn import countplot,lineplot, barplot\nfrom numba import jit\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn import preprocessing\nfrom scipy.stats import randint as sp_randint\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import LeaveOneGroupOut\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import kurtosis, skew\n\nimport matplotlib.style as style\nstyle.use('ggplot')\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport gc\ngc.enable()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[Markus F](https://www.kaggle.com/friedchips)'s work \n[The Missing Link](https://www.kaggle.com/friedchips/the-missing-link) show us there are links between train and test data.\nSame run by robot was recored and splited into train/test.\nBy calculate the squared euclidean distance between two samples, we can find series that link to each other."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#\ndef sq_dist(a, b):\n    ''' the squared euclidean distance between two samples '''\n\n    return np.sum((a - b) ** 2, axis=1)\n\n\ndef find_run_edges(data, edge):\n    ''' examine links between samples. left/right run edges are those samples which do not have a link on that side. '''\n\n    if edge == 'left':\n        border1 = 0\n        border2 = -1\n    elif edge == 'right':\n        border1 = -1\n        border2 = 0\n    else:\n        return False\n\n    edge_list = []\n    linked_list = []\n\n    for i in range(len(data)):\n        dist_list = sq_dist(data[i, border1, :4], data[:, border2, :4])  # distances to rest of samples\n        min_dist = np.min(dist_list)\n        closest_i = np.argmin(dist_list)  # this is i's closest neighbor\n        if closest_i == i:  # this might happen and it's definitely wrong\n            print('Sample', i, 'linked with itself. Next closest sample used instead.')\n            closest_i = np.argsort(dist_list)[1]\n        dist_list = sq_dist(data[closest_i, border2, :4], data[:, border1, :4])  # now find closest_i's closest neighbor\n        rev_dist = np.min(dist_list)\n        closest_rev = np.argmin(dist_list)  # here it is\n        if closest_rev == closest_i:  # again a check\n            print('Sample', i, '(back-)linked with itself. Next closest sample used instead.')\n            closest_rev = np.argsort(dist_list)[1]\n        if (i != closest_rev):  # we found an edge\n            edge_list.append(i)\n        else:\n            linked_list.append([i, closest_i, min_dist])\n\n    return edge_list, linked_list\n\n\ndef find_runs(data, left_edges, right_edges):\n    import time\n    ''' go through the list of samples & link the closest neighbors into a single run '''\n\n    data_runs = []\n\n    for start_point in left_edges:\n        i = start_point\n        run_list = [i]\n        while i not in right_edges:\n            tmp = np.argmin(sq_dist(data[i, -1, :4], data[:, 0, :4]))\n            if tmp == i:  # self-linked sample\n                tmp = np.argsort(sq_dist(data[i, -1, :4], data[:, 0, :4]))[1]\n            i = tmp\n            run_list.append(i)\n        data_runs.append(np.array(run_list))\n\n    return data_runs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we load the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X = pd.read_csv('../input/X_train.csv')\ntest_X  = pd.read_csv('../input/X_test.csv' )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"test_X's series_id shoud be considered as continuous number."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_X['series_id'].tail()) # train_X series_id stop in 3809\ntest_X['series_id'] = test_X['series_id'] + 3810 # so test_X's series_id should start from 3810","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"combine train and test data, and check how many series_id in total"},{"metadata":{"trusted":true},"cell_type":"code","source":"_total = pd.concat([train_X, test_X], axis=0).reset_index(drop=True)\nprint('total series' , len(_total['series_id'].unique()))\ntotal = _total.iloc[:,3:].values.reshape(-1,128,10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we have 7626 series in total, Now we check how many runs we can find, and how many series we used."},{"metadata":{"trusted":true},"cell_type":"code","source":"all_left_edges, all_left_link = find_run_edges(total, edge='left')\nall_right_edges, all_right_link = find_run_edges(total, edge='right')\nprint('Found', len(all_left_edges), 'left edges and', len(all_right_edges), 'right edges.')\n\nall_runs = find_runs(total, all_left_edges, all_right_edges)\n\nflat_list = [series_id for run in all_runs for series_id in run]\nprint(len(flat_list), len(np.unique(flat_list)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we lost 7626-7600=26 series, we find it, and make it into one run, then append it to all_runs"},{"metadata":{"trusted":true},"cell_type":"code","source":"lost_samples = np.array([ i for i in range(len(total)) if i not in np.concatenate(all_runs) ])\nprint(lost_samples)\nprint(len(lost_samples))\n\nlost_run = np.array(lost_samples[find_runs(total[lost_samples], [0], [5])[0]])\nall_runs.append(lost_run)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now we name a Dataframe 'final',it shows run_id/run_pos by series_id."},{"metadata":{"trusted":true},"cell_type":"code","source":"final = pd.DataFrame(index=_total['series_id'].unique())\nfor run_id in range(len(all_runs)):\n    for run_pos in range(len(all_runs[run_id])):\n        series_id = all_runs[run_id][run_pos]\n        final.at[ series_id, 'run_id'  ] = run_id\n        final.at[ series_id, 'run_pos' ] = run_pos\n\ntrain_y = pd.read_csv('../input/y_train.csv')\nfinal['surface'] = train_y['surface']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here is what I did wrong, each run may contain more than one surface type.\nBut here I made same run equals same surface.\nFrankly, I do not know what to do with same run with different surface."},{"metadata":{"trusted":true},"cell_type":"code","source":"for id, surface in zip(final[final['run_id'].notnull()]['run_id'], final[final['surface'].notnull()]['surface']):\n    final.loc[final['run_id']==id, 'surface'] = surface\n\n\nprint(final['run_id'].unique())\nprint(final[final['surface'].isnull()]['run_id'].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now I have new train/test target"},{"metadata":{"trusted":true},"cell_type":"code","source":"new_target = final[final['surface'].notnull()]\nnew_train_series = final[final['surface'].notnull()].index\nnew_test_series = final[final['surface'].isnull()]['run_id'].index\nnew_train = _total[_total['series_id'].isin(new_train_series)]\nnew_test = _total[_total['series_id'].isin(new_test_series)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we do some feature egineering.\n[\"The Orientation Sensor\" or \"Science vs. Alchemy\" discussion](https://www.kaggle.com/c/career-con-2019/discussion/87239#latest-512162)\n[Smart Robots. Complete Notebook](https://www.kaggle.com/jesucristo/1-smart-robots-complete-notebook-0-73)."},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef feat_eng(data):\n    df = pd.DataFrame()\n    data['totl_anglr_vel'] = (data['angular_velocity_X'] ** 2 + data['angular_velocity_Y'] ** 2 + data[\n        'angular_velocity_Z'] ** 2) ** 0.5\n    data['totl_linr_acc'] = (data['linear_acceleration_X'] ** 2 + data['linear_acceleration_Y'] ** 2 + data[\n        'linear_acceleration_Z'] ** 2) ** 0.5\n    data['acc_vs_vel'] = data['totl_linr_acc'] / data['totl_anglr_vel']\n\n    def mean_change_of_abs_change(x):\n        return np.mean(np.diff(np.abs(np.diff(x))))\n\n    for col in data.columns:\n\n        if col in ['row_id', 'series_id', 'measurement_number',\n                   'orientation_X', 'orientation_Y', 'orientation_Z',\n                   'run_id', 'orientation_W']:\n            continue\n        df[col + '_mean'] = data.groupby(['series_id'])[col].mean()\n        df[col + '_median'] = data.groupby(['series_id'])[col].median()\n        df[col + '_max'] = data.groupby(['series_id'])[col].max()\n        df[col + '_min'] = data.groupby(['series_id'])[col].min()\n        df[col + '_std'] = data.groupby(['series_id'])[col].std()\n\n        df[col + '_range'] = df[col + '_max'] - df[col + '_min']\n        df[col + '_maxtoMin'] = df[col + '_max'] / df[col + '_min']\n        df[col + '_mean_abs_chg'] = data.groupby(['series_id'])[col].apply(lambda x: np.mean(np.abs(np.diff(x))))\n        df[col + '_mean_change_of_abs_change'] = data.groupby('series_id')[col].apply(mean_change_of_abs_change)\n        df[col + '_abs_max'] = data.groupby(['series_id'])[col].apply(lambda x: np.max(np.abs(x)))\n        df[col + '_abs_min'] = data.groupby(['series_id'])[col].apply(lambda x: np.min(np.abs(x)))\n        df[col + '_abs_avg'] = (df[col + '_abs_min'] + df[col + '_abs_max']) / 2\n\n\n        df[col + '_kurtosis'] = data.groupby(['series_id'])[col].apply(lambda x:kurtosis(x))\n\n        df[col + '_skew'] = data.groupby(['series_id'])[col].skew()\n\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are leakage in orientation's data.So I'm not using it."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = feat_eng(new_train).reset_index(drop=True)\ntest = feat_eng(new_test).reset_index(drop=True)\n\ndata = data.fillna(0)\ntest = test.fillna(0)\ndata = data.replace(-np.inf,0)\ndata = data.replace(np.inf,0)\ntest = test.replace(-np.inf,0)\ntest = test.replace(np.inf,0)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"surface string into class number"},{"metadata":{"trusted":true},"cell_type":"code","source":"new_target['surface'] = le.fit_transform(new_target['surface'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I use RandomForest, and I try Xgboost and have same public score as RF without tunning.\nI should stack this 2 result, but it's too late for me, I ran out my submission chance."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('modelling')\nmodel = RandomForestClassifier(n_estimators=500, max_depth=10, min_samples_split=5, n_jobs=-1)\nmodel.fit(data.values, new_target['surface'].values)\nmeasured = model.predict(data.values)\npredicted = model.predict_proba(test)\nscore = model.score(data.values, new_target['surface'].values)\nprint(\"score: {}\".format(model.score(data.values, new_target['surface'].values)))\nimportances = model.feature_importances_\nindices = np.argsort(importances)\nfeatures = data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ngc.collect()\n\nprint('Accuracy RF', score )\n\nresult = pd.DataFrame(data={'surface':le.inverse_transform(predicted.argmax(axis=1))},index= new_test['series_id'].unique())\nnew_target['surface'] = le.inverse_transform(new_target['surface'])\n\ndf = pd.concat([result, new_target[['surface']]], axis=0)\nsub = pd.read_csv('../input/sample_submission.csv')\nsub['surface'] = new_target['surface']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('my_submission.csv', index=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sub.head())","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}